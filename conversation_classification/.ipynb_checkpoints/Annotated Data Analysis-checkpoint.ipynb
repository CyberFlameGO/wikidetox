{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import csv\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ids(df):\n",
    "    conversations = df.values.tolist()\n",
    "    conversation_ids = []\n",
    "    for conv in conversations:\n",
    "        cur_conv = json.loads(conv)\n",
    "        conversation_ids.append(list(cur_conv.keys())[0])\n",
    "    return conversation_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def non_readable_or_not_sure(df):\n",
    "    df['not_English'] = (df['na'] == True)\n",
    "    df['not_sure_toxicity'] = (df['toxic'] == -1)\n",
    "    non_english = pd.DataFrame({'not_readable': df.groupby('_unit_id')['not_English'].mean() >=0.5}).reset_index()\n",
    "    non_english = non_english[non_english['not_readable'] == True]['_unit_id'].values.tolist()\n",
    "    not_sure_toxicity = pd.DataFrame({'not_sure_toxicity': df.groupby('_unit_id')['not_sure_toxicity'].mean() >= 0.5}).reset_index()\n",
    "    not_sure_toxicity = not_sure_toxicity[not_sure_toxicity['not_sure_toxicity'] == True]['_unit_id'].values.tolist()\n",
    "    return non_english + not_sure_toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_row_to_coincidence(o, row, columns):\n",
    "    m_u = row.sum(1)\n",
    "    for i in columns:\n",
    "        for j in columns:\n",
    "            if i == j:\n",
    "                o[i][j] = o[i][j] + row[i]*(row[i]-1)/(m_u-1)\n",
    "            else:\n",
    "                o[i][j] = o[i][j] + row[i]*row[j]/(m_u-1)\n",
    "    return o\n",
    "\n",
    "def make_coincidence_matrix(df, columns):\n",
    "    df = df[columns]\n",
    "    n = df.shape[0]\n",
    "    num_cols = len(columns)\n",
    "    o = pd.DataFrame(np.zeros((num_cols,num_cols)), index = columns, columns=columns)\n",
    "    for i in range(n):\n",
    "        o = add_row_to_coincidence(o, df[i:i+1], columns)\n",
    "    return o\n",
    "\n",
    "def binary_distance(i,j):\n",
    "    return i!=j\n",
    "\n",
    "def interval_distance(i,j):\n",
    "    return (int(i)-int(j))**2\n",
    "\n",
    "def e(n, i, j):\n",
    "    if i == j:\n",
    "        return n[i]*(n[i]-1)/sum(n)-1\n",
    "    else:\n",
    "        return n[i]*n[j]/sum(n)-1\n",
    "\n",
    "def D_e(o, columns, distance):\n",
    "    n = o.sum(1)\n",
    "    output = 0\n",
    "    for i in columns:\n",
    "        for j in columns:\n",
    "            output = output + e(n,i,j)*distance(i,j)\n",
    "    return output\n",
    "\n",
    "def D_o(o, columns, distance):\n",
    "    output = 0\n",
    "    for i in columns:\n",
    "        for j in columns:\n",
    "            output = output + o[i][j]*distance(i,j)\n",
    "    return output\n",
    "\n",
    "def Krippendorf_alpha(df, columns, distance = binary_distance, o = None):\n",
    "    if o is None:\n",
    "        o = make_coincidence_matrix(df, columns)\n",
    "    d_o = D_o(o, columns, distance)\n",
    "    d_e = D_e(o, columns, distance)\n",
    "    return (1 - d_o/d_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkEachLineCount(mat):\n",
    "    \"\"\" Assert that each line has a constant number of ratings\n",
    "        @param mat The matrix checked\n",
    "        @return The number of ratings\n",
    "        @throws AssertionError If lines contain different number of ratings \"\"\"\n",
    "    n = sum(mat[0])\n",
    "    \n",
    "    assert all(sum(line) == n for line in mat[1:]), \"Line count != %d (n value).\" % n\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeKappa(mat):\n",
    "    \"\"\" Computes the Kappa value\n",
    "        @param n Number of rating per subjects (number of human raters)\n",
    "        @param mat Matrix[subjects][categories]\n",
    "        @return The Kappa value \"\"\"\n",
    "    n = checkEachLineCount(mat)   # PRE : every line count must be equal to n\n",
    "    N = len(mat)\n",
    "    k = len(mat[0])\n",
    "    \n",
    "    \n",
    "    # Computing p[]\n",
    "    p = [0.0] * k\n",
    "    for j in range(k):\n",
    "        p[j] = 0.0\n",
    "        for i in range(N):\n",
    "            p[j] += mat[i][j]\n",
    "        p[j] /= N*n\n",
    "    \n",
    "    # Computing P[]    \n",
    "    P = [0.0] * N\n",
    "    for i in range(N):\n",
    "        P[i] = 0.0\n",
    "        for j in range(k):\n",
    "            P[i] += mat[i][j] * mat[i][j]\n",
    "        P[i] = (P[i] - n) / (n * (n - 1))\n",
    "  #  if DEBUG: print \"P =\", P\n",
    "    \n",
    "    # Computing Pbar\n",
    "    Pbar = sum(P) / N\n",
    "  #  if DEBUG: print \"Pbar =\", Pbar\n",
    "    \n",
    "    # Computing PbarE\n",
    "    PbarE = 0.0\n",
    "    for pj in p:\n",
    "        PbarE += pj * pj\n",
    "   # if DEBUG: print \"PbarE =\", PbarE\n",
    "    \n",
    "    kappa = (Pbar - PbarE) / (1 - PbarE)\n",
    "    print(PbarE, Pbar)\n",
    "    #if DEBUG: print \"kappa =\", kappa\n",
    "    \n",
    "    return kappa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOB 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/scratch/wiki_dumps/annotated/job1_constraintsAB_v017.csv') as f:\n",
    "     df = pd.read_csv(f, encoding = 'utf-8', index_col=None, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "toxicity = df[df['toxic'] >= 0]\n",
    "toxic_before = pd.DataFrame({'toxic_before': toxicity.groupby('_unit_id')['toxic'].mean() >= 0.5}).\\\n",
    "            reset_index()\n",
    "toxic_before = toxic_before[toxic_before['toxic_before'] == True]['_unit_id'].values.tolist()\n",
    "_excluded = list(set(non_readable_or_not_sure(df) + toxic_before))\n",
    "annotated = set(get_ids(df['conversations']))\n",
    "excluded_1 = set(get_ids(df[df['_unit_id'].isin(_excluded)]['conversations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TOXIC_COLUMNS = ['no_toxic', 'toxic']\n",
    "\n",
    "no_toxic = pd.DataFrame({'no_toxic': df[df['toxic'] == 0].groupby('_unit_id').size()}).reset_index().set_index('_unit_id')\n",
    "toxic = pd.DataFrame({'toxic': df[df['toxic'] == 1].groupby('_unit_id').size()}).reset_index().set_index('_unit_id')\n",
    "total = toxic.join(no_toxic, how='outer')\n",
    "total = total.fillna(int(0)).reset_index()\n",
    "total['sum'] = total['toxic'] + total['no_toxic']\n",
    "#total = total[total['sum'] ==20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5574067964368196"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total[total['sum'] >=20]) / len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = total[total['sum'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total['toxic'] = total.apply(lambda x: int(x['toxic']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1111 -0.0204549600356\n"
     ]
    }
   ],
   "source": [
    "print(len(total), Krippendorf_alpha(total, TOXIC_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8499687505510296 0.8678006503962459\n",
      "0.11885457136902251\n",
      "MannwhitneyuResult(statistic=5491940.5, pvalue=0.19181043767508715)\n",
      "0.8499687505510296 0.8660452120102394\n",
      "0.10715408635370896\n"
     ]
    }
   ],
   "source": [
    "mat = []\n",
    "for index, row in total.iterrows():\n",
    "    mat.append([int(row['toxic']), int(row['no_toxic'])])\n",
    "\n",
    "import copy\n",
    "\n",
    "randomized_mat = copy.deepcopy(mat)\n",
    "l = len(randomized_mat) - 1\n",
    "for i in range(50000):\n",
    "    x = random.randint(0, l)\n",
    "    y = random.randint(0, l)\n",
    "    xx = random.randint(0, 1)\n",
    "    yy = random.randint(0, 1)\n",
    "    if randomized_mat[x][xx] > 0 and randomized_mat[y][yy] > 0:\n",
    "        randomized_mat[x][xx] -= 1\n",
    "        randomized_mat[y][xx] += 1\n",
    "        randomized_mat[y][yy] -= 1\n",
    "        randomized_mat[x][yy] += 1\n",
    "\n",
    "#np.transpose(mat)[0]\n",
    "\n",
    "print(computeKappa(mat))\n",
    "\n",
    "#np.transpose(randomized_mat)[0]\n",
    "\n",
    "#np.transpose(randomized_mat)[1]\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(randomized_mat)):\n",
    "    x.append(max(randomized_mat[i][0], randomized_mat[i][1]))\n",
    "    y.append(max(mat[i][0], mat[i][1]))\n",
    "    \n",
    "print(scipy.stats.mannwhitneyu(x, y, alternative='less'))\n",
    "\n",
    "print(computeKappa(randomized_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total['sum'] = total['toxic'] + total['no_toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method FramePlotMethods.hist of <pandas.plotting._core.FramePlotMethods object at 0x7f2a616362e8>>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total[['sum']].plot.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n,bins,batches = plt.hist(total['no_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9871025794841032"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total[total['no_toxic'] > 20]) / len(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npI4ZApLUMUNAkjpmCEhSx/4/BM+qJ7on7vMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a6407ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5950 0.0861351020084\n"
     ]
    }
   ],
   "source": [
    "total = total[~(total['_unit_id'].isin(_excluded))]\n",
    "\n",
    "print(len(total), Krippendorf_alpha(total, TOXIC_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each Conversation is annotated by 5~22 workers JOB1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOB 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/scratch/wiki_dumps/annotated/job2_constraintsAB_v017.csv') as f:\n",
    "     df = pd.read_csv(f, encoding = 'utf-8', index_col=None, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "_excluded = list(set(non_readable_or_not_sure(df)))\n",
    "excluded_2 = set(get_ids(df[df['_unit_id'].isin(_excluded)]['conversations']))\n",
    "excluded = (excluded_1 | excluded_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['conv_id'] = df.apply(lambda x: list(json.loads(x['conversations']).keys())[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxicity = df[df['toxic'] >= 0]\n",
    "toxic = pd.DataFrame({'toxic': toxicity.groupby('_unit_id')['toxic'].mean() >= 0.5}).reset_index()\n",
    "bad_convs = toxic[toxic['toxic'] == True]['_unit_id'].values.tolist()\n",
    "bad_conversations = set(get_ids(df[df['_unit_id'].isin(bad_convs)]['conversations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2643 0.282819056548\n"
     ]
    }
   ],
   "source": [
    "TOXIC_COLUMNS = ['no_toxic', 'toxic']\n",
    "\n",
    "no_toxic = pd.DataFrame({'no_toxic': df[df['toxic'] == 0].groupby('_unit_id').size()}).reset_index().set_index('_unit_id')\n",
    "toxic = pd.DataFrame({'toxic': df[df['toxic'] == 1].groupby('_unit_id').size()}).reset_index().set_index('_unit_id')\n",
    "total = toxic.join(no_toxic, how='outer')\n",
    "total = total.fillna(0).reset_index()\n",
    "total['sum'] = total['toxic'] + total['no_toxic']\n",
    "total = total[total['sum'] ==20]\n",
    "print(len(total), Krippendorf_alpha(total, TOXIC_COLUMNS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5044058449213501 0.6445964514009275\n",
      "0.282873809230719\n",
      "MannwhitneyuResult(statistic=3741727.5, pvalue=0.99999674764091862)\n",
      "0.5044058449213501 0.6601131091064856\n",
      "0.3141830116225342\n"
     ]
    }
   ],
   "source": [
    "mat = []\n",
    "for index, row in total.iterrows():\n",
    "    mat.append([int(row['toxic']), int(row['no_toxic'])])\n",
    "\n",
    "import copy\n",
    "\n",
    "randomized_mat = copy.deepcopy(mat)\n",
    "l = len(randomized_mat) - 1\n",
    "for i in range(70000):\n",
    "    x = random.randint(0, l)\n",
    "    y = random.randint(0, l)\n",
    "    xx = random.randint(0, 1)\n",
    "    yy = random.randint(0, 1)\n",
    "    if randomized_mat[x][xx] > 0 and randomized_mat[y][yy] > 0:\n",
    "        randomized_mat[x][xx] -= 1\n",
    "        randomized_mat[y][xx] += 1\n",
    "        randomized_mat[y][yy] -= 1\n",
    "        randomized_mat[x][yy] += 1\n",
    "\n",
    "#np.transpose(mat)[0]\n",
    "\n",
    "print(computeKappa(mat))\n",
    "\n",
    "#np.transpose(randomized_mat)[0]\n",
    "\n",
    "#np.transpose(randomized_mat)[1]\n",
    "x = []\n",
    "y = []\n",
    "for i in range(len(randomized_mat)):\n",
    "    x.append(max(randomized_mat[i][0], randomized_mat[i][1]))\n",
    "    y.append(max(mat[i][0], mat[i][1]))\n",
    "    \n",
    "print(scipy.stats.mannwhitneyu(x, y, alternative='less'))\n",
    "\n",
    "print(computeKappa(randomized_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Krippendorf_alpha(total, TOXIC_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['has_disagreement'] = ~(df['disagreement'] == None)\n",
    "disagreement = pd.DataFrame({'disagreement': df.groupby('_unit_id')['has_disagreement'].mean() >= 0.5}).reset_index()\n",
    "disagree_convs = disagreement[disagreement['disagreement'] == True]['_unit_id'].values.tolist()\n",
    "disagree_conversations = set(get_ids(df[df['_unit_id'].isin(disagree_convs)]['conversations']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bad_and_disagree = (bad_conversations & disagree_conversations - excluded)\n",
    "good_and_disagree = (disagree_conversations - bad_conversations - excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attacker_in_conv(conv):\n",
    "    actions = conv['action_feature']\n",
    "    end_time = max([a['timestamp_in_sec'] for a in actions])\n",
    "    attacker = None\n",
    "    for a in actions:\n",
    "        if a['timestamp_in_sec'] == end_time:\n",
    "            attacker = a['user_text']\n",
    "    for a in actions:\n",
    "        if a['timestamp_in_sec'] < end_time and a['user_text'] == attacker:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goods = {}\n",
    "bads = []\n",
    "all_data = {}\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/all.json') as f:\n",
    "    for line in f:\n",
    "        conv_id, clss, conversation = json.loads(line)\n",
    "        all_data[conv_id] = [conv_id, clss, conversation]\n",
    "        if clss == 0 and (attacker_in_conv(all_data[conversation['action_feature'][0]['good_conversation_id']][2])):\n",
    "            bads.append([conv_id, clss, conversation])\n",
    "            goods[conv_id] = all_data[conversation['action_feature'][0]['good_conversation_id']]\n",
    "cleaned_data = bads + list(goods.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(cleand_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "bad_disagree = []\n",
    "good_disagree = []\n",
    "xs = []\n",
    "total_bad = 0\n",
    "total_good = 0\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/all.json') as f:\n",
    "    for line in f:\n",
    "        conv_id, clss, conversation = json.loads(line)\n",
    "        idlist = [k['id'] for k in conversation['action_feature']]\n",
    "        is_annotated = not(annotated.isdisjoint(idlist))\n",
    "        is_excluded = not(excluded.isdisjoint(idlist))\n",
    "        is_bd = not(bad_and_disagree.isdisjoint(idlist))\n",
    "        is_gd = not(good_and_disagree.isdisjoint(idlist))\n",
    "        annotated_class = bad_conversations.isdisjoint(idlist)\n",
    "        same_class = (int(annotated_class) == clss)\n",
    "        if 'good_conversation_id' in conversation['action_feature'][0]:\n",
    "            matched_id = conversation['action_feature'][0]['good_conversation_id']\n",
    "        else:\n",
    "            matched_id = conversation['action_feature'][0]['bad_conversation_id']\n",
    "        dt = {'conversation_id': conv_id, 'annotated': is_annotated, 'excluded': is_excluded, \\\n",
    "                     'annotated_class': annotated_class, 'class': clss, 'matched_id': matched_id, 'same_class': same_class}\n",
    "        if is_bd and not(is_excluded) and is_annotated:\n",
    "            bad_disagree.append(dt)\n",
    "        if is_gd and not(is_excluded):\n",
    "            good_disagree.append(dt)\n",
    "        if annotated_class:\n",
    "            total_good += 1\n",
    "        else:total_bad += 1\n",
    "        data.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1668\n",
      "1720\n",
      "0.9697674418604652\n"
     ]
    }
   ],
   "source": [
    "bad_but_disagree = pd.DataFrame(bad_disagree)\n",
    "print(len(bad_but_disagree))\n",
    "print(total_bad)\n",
    "print(len(bad_but_disagree) / total_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3602\n",
      "4744\n",
      "0.7592748735244519\n"
     ]
    }
   ],
   "source": [
    "good_but_disagree = pd.DataFrame(good_disagree)\n",
    "\n",
    "print(len(good_but_disagree))\n",
    "print(total_good)\n",
    "print(len(good_but_disagree) / total_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31650853889943076"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1668/(1668+3602) # toxic&disagree / all disagree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2660891089108911"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1720/(1720+4744) # toxic / all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotation_aggregated = pd.DataFrame(data)\n",
    "class_bad_annotation_good = annotation_aggregated[(annotation_aggregated['annotated'] == True)\\\n",
    "                                              & (annotation_aggregated['excluded'] == False)\\\n",
    "                                              & (annotation_aggregated['same_class'] == False)\\\n",
    "                                              & (annotation_aggregated['class'] == False)]\n",
    "annotation_aggregated = annotation_aggregated[(annotation_aggregated['annotated'] == True)\\\n",
    "                                              & (annotation_aggregated['excluded'] == False)\\\n",
    "                                              & (annotation_aggregated['same_class'] == True)]\n",
    "left_ids = annotation_aggregated['conversation_id'].values.tolist()\n",
    "annotation_aggregated = annotation_aggregated[annotation_aggregated['matched_id'].isin(left_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verified_pairs = annotation_aggregated['conversation_id'].values.tolist()\n",
    "goods = {}\n",
    "bads = []\n",
    "all_data = {}\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/all_verified.json', 'w') as w:\n",
    "    with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/all.json') as f:\n",
    "        for line in f:\n",
    "            conv_id, clss, conversation = json.loads(line)\n",
    "            if conv_id in verified_pairs:\n",
    "                w.write(json.dumps([conv_id, clss, conversation]) + '\\n')\n",
    "                all_data[conv_id] = [conv_id, clss, conversation]\n",
    "                if clss == 0:\n",
    "                    bads.append([conv_id, clss, conversation])\n",
    "                    goods[conv_id] = all_data[conversation['action_feature'][0]['good_conversation_id']]\n",
    "                    if conv_id ==  '55624552.2155.2155': print(conv_id, goods[conv_id][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "verified_pairs = annotation_aggregated['conversation_id'].values.tolist()\n",
    "goods = {}\n",
    "bads = []\n",
    "all_data = {}\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/all_verified_cleaned.json', 'w') as w:\n",
    "    with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/all.json') as f:\n",
    "        for line in f:\n",
    "            conv_id, clss, conversation = json.loads(line)\n",
    "            if conv_id in verified_pairs:\n",
    "                w.write(json.dumps([conv_id, clss, conversation]) + '\\n')\n",
    "                all_data[conv_id] = [conv_id, clss, conversation]\n",
    "                if clss == 0 and (attacker_in_conv(all_data[conversation['action_feature'][0]['good_conversation_id']][2])):\n",
    "                    bads.append([conv_id, clss, conversation])\n",
    "                    goods[conv_id] = all_data[conversation['action_feature'][0]['good_conversation_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " '55624552.2155.2155' in bads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = {}\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/bak.json') as f:\n",
    "    for line in f:\n",
    "        conv_id, clss, conversation = json.loads(line)\n",
    "        test[conv_id] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1492"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left = []\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/test_verified.json', 'w') as w:\n",
    "    for conv in bads:\n",
    "        conv_id, clss, conversation = conv\n",
    "        if not(conv_id in test):\n",
    "            left.append(conv)\n",
    "            left.append(goods[conv_id])\n",
    "        else:\n",
    "            w.write(json.dumps(conv) + '\\n')\n",
    "            w.write(json.dumps(goods[conv_id]) + '\\n')\n",
    "\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/train_verified.json', 'w') as w:\n",
    "    for conv in left:\n",
    "        w.write(json.dumps(conv) + '\\n')\n",
    "    #    w.write(json.dumps(goods[conv[0]]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "left = []\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/test_cleaned_verified.json', 'w') as w:\n",
    "    for conv in bads:\n",
    "        conv_id, clss, conversation = conv\n",
    "        if not(conv_id in test):\n",
    "            left.append(conv)\n",
    "            left.append(goods[conv_id])\n",
    "        else:\n",
    "            w.write(json.dumps(conv) + '\\n')\n",
    "            w.write(json.dumps(goods[conv_id]) + '\\n')\n",
    "\n",
    "with open('/scratch/wiki_dumps/expr_with_matching/delta2_no_users_attacker_in_conv/data/train_cleaned_verified.json', 'w') as w:\n",
    "    for conv in left:\n",
    "        w.write(json.dumps(conv) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1314.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotation_aggregated) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reannotation_needed = class_bad_annotation_good['conversation_id'].values.tolist()\n",
    "with open(\"toxicity_in_context.json\", 'w') as f:\n",
    "    json.dump(reannotation_needed, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__': \n",
    "    print(\"Example from http://en.wikipedia.org/wiki/Krippendorff's_Alpha\")\n",
    "\n",
    "    data = (\n",
    "        \"*    *    *    *    *    3    4    1    2    1    1    3    3    *    3\", # coder A\n",
    "        \"1    *    2    1    3    3    4    3    *    *    *    *    *    *    *\", # coder B\n",
    "        \"*    *    2    1    3    4    4    *    2    1    1    3    3    *    4\", # coder C\n",
    "    )\n",
    "\n",
    "    missing = '*' # indicator for missing values\n",
    "    array = [d.split() for d in data]  # convert to 2D list of string items\n",
    "    \n",
    "    print(\"nominal metric: %.3f\" % krippendorff_alpha(array, nominal_metric, missing_items=missing))\n",
    "    print(\"interval metric: %.3f\" % krippendorff_alpha(array, interval_metric, missing_items=missing))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
